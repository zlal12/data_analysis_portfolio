# -*- coding: utf-8 -*-
"""H&M_What's your next item.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sKKyx-RcMOm65bw85VlpU8X9icn40kvv

# **Import the libraries and csv files**
"""

# Import the libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import networkx as nx
from gensim.models import Word2Vec
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.image as mpimg
import random

# Connect google drive
from google.colab import drive
drive.mount('/content/drive')

# Read the csv files
articles = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/data/h-and-m-personalized-fashion-recommendations/articles.csv")
transactions = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/data/h-and-m-personalized-fashion-recommendations/transactions_train.csv")
customers = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/data/h-and-m-personalized-fashion-recommendations/customers.csv")
colors = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/data/h-and-m-personalized-fashion-recommendations/colors.csv")

"""# **Check the data**"""

articles.info()
articles.dtypes

transactions.info()
transactions.dtypes

customers.info()
customers.dtypes

colors.info()
colors.dtypes

"""# **Preprocessing**"""

# Filter datasets for performance
item_freq = transactions['article_id'].value_counts()
user_freq = transactions['customer_id'].value_counts()

filtered_items = item_freq[item_freq >= 100].index
filtered_users = user_freq[user_freq >= 100].index

filtered_df = transactions[transactions['article_id'].isin(filtered_items) & transactions['customer_id'].isin(filtered_users)]

freq = filtered_df.groupby(['customer_id', 'article_id']).size().reset_index(name='frequency')

GraphTravel_HM = filtered_df.merge(freq, on=['customer_id', 'article_id'], how='left')

GraphTravel_HM = GraphTravel_HM[GraphTravel_HM['frequency'] >= 10]

# Check the dataset after controling weight and search bias
display(GraphTravel_HM)

print("unique customer_id" , GraphTravel_HM.customer_id.nunique())
print("unique article_id" , GraphTravel_HM.article_id.nunique())

# Distribution of frequency
sns.distplot(GraphTravel_HM['frequency'], kde=True, bins=30)

plt.title('Distribution of frequency')
plt.xlabel('Frequency')
plt.ylabel('Density')

plt.show()

# Add 'personal_color' column to customers dataset
personal_colors = ['spring', 'summer', 'autumn', 'winter']
customers['personal_color'] = np.random.choice(personal_colors, size=len(customers))
customers.head()

# Merge articles and colors data on 'colour_group_name' and 'color_name' from the colors file
color_mapping = colors[['color_name', 'color_type']].rename(columns={'color_name': 'colour_group_name'})
recommendation_data = pd.merge(articles, color_mapping, how='left', on='colour_group_name')

# Merge with customers data to associate customer personal color with articles
recommendation_data = pd.merge(recommendation_data, customers[['customer_id', 'personal_color']],
                               how='left', left_on='color_type', right_on='personal_color')

print(recommendation_data.head())

# Customer ID mapping
unique_customer_ids = GraphTravel_HM['customer_id'].unique()
customer_id_mapping = {id: i for i, id in enumerate(unique_customer_ids)}
GraphTravel_HM['customer_id'] = GraphTravel_HM['customer_id'].map(customer_id_mapping)

item_name_mapping = dict(zip(articles['article_id'], articles['prod_name']))

"""# Make a item recommendation model"""

G = nx.Graph()

for index, row in GraphTravel_HM.iterrows():
    G.add_node(row['customer_id'], type='user')
    G.add_node(row['article_id'], type='item')
    G.add_edge(row['customer_id'], row['article_id'], weight=row['frequency'])

# Biased random walk
def biased_random_walk(G, start_node, walk_length, p=1, q=1):
    walk = [start_node]

    while len(walk) < walk_length:
        cur_node = walk[-1]
        cur_neighbors = list(G.neighbors(cur_node))

        if len(cur_neighbors) > 0:
            if len(walk) == 1:
                walk.append(random.choice(cur_neighbors))
            else:
                prev_node = walk[-2]

                probability = []
                for neighbor in cur_neighbors:
                    if neighbor == prev_node:
                        # Return parameter
                        probability.append(1/p)
                    elif G.has_edge(neighbor, prev_node):
                        # Stay parameter
                        probability.append(1)
                    else:
                        # In-out parameter
                        probability.append(1/q)

                probability = np.array(probability)
                probability = probability / probability.sum()  # normalize

                next_node = np.random.choice(cur_neighbors, p=probability)
                walk.append(next_node)
        else:
            break

    return walk

# Generate random walks
def generate_walks(G, num_walks, walk_length, p=1, q=1):
    walks = []
    nodes = list(G.nodes())
    for _ in range(num_walks):
        random.shuffle(nodes)  # to ensure randomness
        for node in nodes:
            walk_from_node = biased_random_walk(G, node, walk_length, p, q)
            walks.append(walk_from_node)
    return walks

# Train Word2Vec model
walks = generate_walks(G, num_walks=10, walk_length=20, p=9, q=1)
filtered_walks = [walk for walk in walks if len(walk) >= 5]

# to String  (for Word2Vec input)
walks = [[str(node) for node in walk] for walk in walks]

# Word2Vec train
model = Word2Vec(walks, vector_size=128, window=5, min_count=0,  hs=1, sg=1, workers=4, epochs=10)

# node embedding extract
embeddings = {node_id: model.wv[node_id] for node_id in model.wv.index_to_key}

# Get user embedding
def get_user_embedding(user_id, embeddings):
    return embeddings[str(user_id)]

# Get rated items
def get_rated_items(user_id, df):
    return set(df[df['customer_id'] == user_id]['article_id'])

# Calculate item similarities with personal color filter
def calculate_similarities(user_id, df, embeddings):
    rated_items = get_rated_items(user_id, df)
    user_embedding = get_user_embedding(user_id, embeddings)

    item_similarities = []
    for item_id in set(df['article_id']):
        if item_id not in rated_items:
            item_embedding = embeddings[str(item_id)]
            similarity = cosine_similarity([user_embedding], [item_embedding])[0][0]
            item_similarities.append((item_id, similarity))

    return item_similarities

def show_images(items, item_name_mapping, num_items, show_similarity=False):
    f, ax = plt.subplots(1, num_items, figsize=(20,10))
    if num_items == 1:
        ax = [ax]
    for i, item in enumerate(items):
        item_id, similarity = item
        print(f"- Item {item_id}: {item_name_mapping[item_id]}", end='')
        if show_similarity:
            print(f" with similarity score: {similarity}")
        else:
            print()
        img_path = f"/content/drive/MyDrive/Colab Notebooks/data/h-and-m-personalized-fashion-recommendations/images/0{str(item_id)[:2]}/0{int(item_id)}.jpg"
        try:
            img = mpimg.imread(img_path)
            ax[i].imshow(img)
            ax[i].set_title(f'Item {item_id}')
            ax[i].set_xticks([], [])
            ax[i].set_yticks([], [])
            ax[i].grid(False)
        except FileNotFoundError:
            print(f"Image for item {item_id} not found.")
    plt.show()

def recommend_items(user_id, df, embeddings, item_name_mapping, recommendation_data, customers, num_items=5):
    # Create a user_id to customer_id mapping
    user_id_to_customer_id = {v: k for k, v in customer_id_mapping.items()}

    # Map the user_id to the corresponding customer_id
    if user_id not in user_id_to_customer_id:
        print(f"User ID {user_id} not found in transactions data.")
        return

    customer_id = user_id_to_customer_id[user_id]
    print(f"Mapped User ID {user_id} to Customer ID: {customer_id}")

    # Retrieve items rated/purchased by the user
    rated_items = get_rated_items(user_id, df)

    print(f"User {user_id} (Customer ID: {customer_id}) has purchased:")
    show_images([(item_id, 0) for item_id in list(rated_items)[:5]], item_name_mapping, min(len(rated_items), 5))

    # Retrieve the personal color of the customer
    if customer_id in customers['customer_id'].values:
        customer_color = customers[customers['customer_id'] == customer_id]['personal_color'].values[0]
        print(f"Customer {customer_id} has personal color: {customer_color}")

        # Normalize color types
        recommendation_data['color_type'] = recommendation_data['color_type'].str.lower()

        # Filter recommendation data by personal color
        filtered_recommendation_data = recommendation_data[recommendation_data['color_type'] == customer_color]
        print(f"Filtering by personal color '{customer_color}' returned {len(filtered_recommendation_data)} items.")

        if not filtered_recommendation_data.empty:
            # Filter recommendations based on customer color
            item_similarities = calculate_similarities(user_id, df, embeddings)
            recommended_items = sorted(item_similarities, key=lambda x: x[1], reverse=True)[:num_items]

            # Apply the personal color filter to recommended items
            recommended_items_filtered = [
                (item_id, similarity) for item_id, similarity in recommended_items
                if item_id in filtered_recommendation_data['article_id'].values
            ]

            if recommended_items_filtered:
                print(f"\nRecommended items for user {user_id} (filtered by personal color {customer_color}):")
                show_images(recommended_items_filtered, item_name_mapping, num_items, show_similarity=True)
            else:
                print(f"No recommendations found for personal color: {customer_color}. Showing unfiltered recommendations.")
                item_similarities = calculate_similarities(user_id, df, embeddings)
                recommended_items = sorted(item_similarities, key=lambda x: x[1], reverse=True)[:num_items]
                show_images(recommended_items, item_name_mapping, num_items, show_similarity=True)
        else:
            print(f"No items with color type '{customer_color}' found. Showing unfiltered recommendations.")
            item_similarities = calculate_similarities(user_id, df, embeddings)
            recommended_items = sorted(item_similarities, key=lambda x: x[1], reverse=True)[:num_items]
            show_images(recommended_items, item_name_mapping, num_items, show_similarity=True)
    else:
        print(f"Customer ID {customer_id} not found in customers data. Skipping personal color filter.")
        item_similarities = calculate_similarities(user_id, df, embeddings)
        recommended_items = sorted(item_similarities, key=lambda x: x[1], reverse=True)[:num_items]
        show_images(recommended_items, item_name_mapping, num_items, show_similarity=True)

recommend_items(
    user_id=18,
    df=GraphTravel_HM,
    embeddings=embeddings,
    item_name_mapping=item_name_mapping,
    recommendation_data=recommendation_data,
    customers=customers,
    num_items=5
)

recommend_items(
    user_id=1018,
    df=GraphTravel_HM,
    embeddings=embeddings,
    item_name_mapping=item_name_mapping,
    recommendation_data=recommendation_data,
    customers=customers,
    num_items=5
)

recommend_items(
    user_id=14,
    df=GraphTravel_HM,
    embeddings=embeddings,
    item_name_mapping=item_name_mapping,
    recommendation_data=recommendation_data,
    customers=customers,
    num_items=5
)

"""# Reference

https://www.kaggle.com/code/khyj00/gnns-similarity-recommendation-based-on-node2vec
"""